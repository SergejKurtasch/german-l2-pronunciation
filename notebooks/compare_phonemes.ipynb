{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison of phonemes from all models\n",
        "\n",
        "This notebook displays all phonemes returned by four models:\n",
        "1. Main model (vitouphy/wav2vec2-xls-r-300m-phoneme) via PhonemeRecognizer\n",
        "2. Wav2Vec2Phoneme model (vitouphy/wav2vec2-xls-r-300m-phoneme) via Wav2Vec2PhonemeRecognizer\n",
        "3. Wav2Vec2 Large model (facebook/wav2vec2-large-960h-lv60-self) via PhonemeRecognizer\n",
        "4. **CommonPhone model (pklumpp/Wav2Vec2_CommonPhone)** - trained on IPA phonemes from Common Phone dataset (includes German)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Determine project root (parent of notebooks directory)\n",
        "# This allows the notebook to work regardless of where the project is located\n",
        "current_dir = Path.cwd()\n",
        "if current_dir.name == 'notebooks':\n",
        "    project_root = current_dir.parent\n",
        "else:\n",
        "    # Fallback: assume we're at project root\n",
        "    project_root = current_dir\n",
        "\n",
        "sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import modules\n",
        "from modules.phoneme_recognition import get_phoneme_recognizer\n",
        "from modules.wav2vec2_phoneme_recognition import get_wav2vec2_phoneme_recognizer\n",
        "import config\n",
        "\n",
        "print(\"Imports completed successfully\")\n",
        ""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Imports completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def filter_phonemes(vocab):\n",
        "    \"\"\"\n",
        "    Filter phonemes from vocabulary, excluding special tokens.\n",
        "    \n",
        "    Args:\n",
        "        vocab: Dictionary {token: id}\n",
        "        \n",
        "    Returns:\n",
        "        List of phonemes (without special tokens)\n",
        "    \"\"\"\n",
        "    # Special tokens to exclude\n",
        "    skip_tokens = {\n",
        "        '[PAD]', '[UNK]', '<pad>', '<unk>', '<blank>', '[BLANK]', \n",
        "        '<s>', '</s>', '<|endoftext|>', '|', 'h#', 'spn', '',\n",
        "        '<sos>', '<eos>', '[CLS]', '[SEP]', '[MASK]'\n",
        "    }\n",
        "    \n",
        "    phonemes = []\n",
        "    for token in vocab.keys():\n",
        "        # Skip special tokens\n",
        "        if token in skip_tokens:\n",
        "            continue\n",
        "        # Skip tokens that look like service tokens\n",
        "        if token.startswith('<') and token.endswith('>'):\n",
        "            continue\n",
        "        if token.startswith('[') and token.endswith(']'):\n",
        "            continue\n",
        "        phonemes.append(token)\n",
        "    \n",
        "    return sorted(phonemes)\n",
        "\n",
        "print(\"Filter function created\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filter function created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load first model (main model)\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL 1: Main model (PhonemeRecognizer)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Model: {config.MODEL_NAME}\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    model1 = get_phoneme_recognizer(\n",
        "        model_name=config.MODEL_NAME,\n",
        "        device=config.MODEL_DEVICE if config.MODEL_DEVICE != \"auto\" else None\n",
        "    )\n",
        "    vocab1 = model1.get_vocab()\n",
        "    phonemes1 = filter_phonemes(vocab1)\n",
        "    \n",
        "    print(f\"Vocabulary size: {len(vocab1)}\")\n",
        "    print(f\"Number of phonemes (after filtering): {len(phonemes1)}\")\n",
        "    print()\n",
        "    print(\"All phonemes from model 1:\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, phoneme in enumerate(phonemes1, 1):\n",
        "        print(f\"{i:3d}. {phoneme}\")\n",
        "    print()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading model 1: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    phonemes1 = []\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "MODEL 1: Main model (PhonemeRecognizer)\n",
            "================================================================================\n",
            "Model: vitouphy/wav2vec2-xls-r-300m-phoneme\n",
            "\n",
            "Vocabulary size: 45\n",
            "Number of phonemes (after filtering): 38\n",
            "\n",
            "All phonemes from model 1:\n",
            "--------------------------------------------------------------------------------\n",
            "  1. aa\n",
            "  2. ae\n",
            "  3. ah\n",
            "  4. aw\n",
            "  5. ay\n",
            "  6. b\n",
            "  7. ch\n",
            "  8. d\n",
            "  9. dh\n",
            " 10. dx\n",
            " 11. eh\n",
            " 12. er\n",
            " 13. ey\n",
            " 14. f\n",
            " 15. g\n",
            " 16. hh\n",
            " 17. ih\n",
            " 18. iy\n",
            " 19. jh\n",
            " 20. k\n",
            " 21. l\n",
            " 22. m\n",
            " 23. n\n",
            " 24. ng\n",
            " 25. ow\n",
            " 26. oy\n",
            " 27. p\n",
            " 28. r\n",
            " 29. s\n",
            " 30. sh\n",
            " 31. t\n",
            " 32. th\n",
            " 33. uh\n",
            " 34. uw\n",
            " 35. v\n",
            " 36. w\n",
            " 37. y\n",
            " 38. z\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load second model (Wav2Vec2Phoneme)\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL 2: Wav2Vec2Phoneme model (Wav2Vec2PhonemeRecognizer)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Model: {config.WAV2VEC2_PHONEME_MODEL_NAME}\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    model2 = get_wav2vec2_phoneme_recognizer(\n",
        "        model_name=config.WAV2VEC2_PHONEME_MODEL_NAME,\n",
        "        device=config.MODEL_DEVICE if config.MODEL_DEVICE != \"auto\" else None\n",
        "    )\n",
        "    vocab2 = model2.get_vocab()\n",
        "    phonemes2 = filter_phonemes(vocab2)\n",
        "    \n",
        "    print(f\"Vocabulary size: {len(vocab2)}\")\n",
        "    print(f\"Number of phonemes (after filtering): {len(phonemes2)}\")\n",
        "    print()\n",
        "    print(\"All phonemes from model 2:\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, phoneme in enumerate(phonemes2, 1):\n",
        "        print(f\"{i:3d}. {phoneme}\")\n",
        "    print()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading model 2: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    phonemes2 = []\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "MODEL 2: Wav2Vec2Phoneme model (Wav2Vec2PhonemeRecognizer)\n",
            "================================================================================\n",
            "Model: vitouphy/wav2vec2-xls-r-300m-phoneme\n",
            "\n",
            "Vocabulary size: 45\n",
            "Number of phonemes (after filtering): 38\n",
            "\n",
            "All phonemes from model 2:\n",
            "--------------------------------------------------------------------------------\n",
            "  1. aa\n",
            "  2. ae\n",
            "  3. ah\n",
            "  4. aw\n",
            "  5. ay\n",
            "  6. b\n",
            "  7. ch\n",
            "  8. d\n",
            "  9. dh\n",
            " 10. dx\n",
            " 11. eh\n",
            " 12. er\n",
            " 13. ey\n",
            " 14. f\n",
            " 15. g\n",
            " 16. hh\n",
            " 17. ih\n",
            " 18. iy\n",
            " 19. jh\n",
            " 20. k\n",
            " 21. l\n",
            " 22. m\n",
            " 23. n\n",
            " 24. ng\n",
            " 25. ow\n",
            " 26. oy\n",
            " 27. p\n",
            " 28. r\n",
            " 29. s\n",
            " 30. sh\n",
            " 31. t\n",
            " 32. th\n",
            " 33. uh\n",
            " 34. uw\n",
            " 35. v\n",
            " 36. w\n",
            " 37. y\n",
            " 38. z\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load third model (Wav2Vec2 Large)\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL 3: Wav2Vec2 Large model (PhonemeRecognizer)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Model: {config.WAV2VEC2_LARGE_MODEL_NAME}\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    model3 = get_phoneme_recognizer(\n",
        "        model_name=config.WAV2VEC2_LARGE_MODEL_NAME,\n",
        "        device=config.MODEL_DEVICE if config.MODEL_DEVICE != \"auto\" else None\n",
        "    )\n",
        "    vocab3 = model3.get_vocab()\n",
        "    phonemes3 = filter_phonemes(vocab3)\n",
        "    \n",
        "    print(f\"Vocabulary size: {len(vocab3)}\")\n",
        "    print(f\"Number of phonemes (after filtering): {len(phonemes3)}\")\n",
        "    print()\n",
        "    print(\"All phonemes from model 3:\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, phoneme in enumerate(phonemes3, 1):\n",
        "        print(f\"{i:3d}. {phoneme}\")\n",
        "    print()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error loading model 3: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    phonemes3 = []\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "MODEL 3: Wav2Vec2 Large model (PhonemeRecognizer)\n",
            "================================================================================\n",
            "Model: facebook/wav2vec2-large-960h-lv60-self\n",
            "\n",
            "Vocabulary size: 32\n",
            "Number of phonemes (after filtering): 27\n",
            "\n",
            "All phonemes from model 3:\n",
            "--------------------------------------------------------------------------------\n",
            "  1. '\n",
            "  2. A\n",
            "  3. B\n",
            "  4. C\n",
            "  5. D\n",
            "  6. E\n",
            "  7. F\n",
            "  8. G\n",
            "  9. H\n",
            " 10. I\n",
            " 11. J\n",
            " 12. K\n",
            " 13. L\n",
            " 14. M\n",
            " 15. N\n",
            " 16. O\n",
            " 17. P\n",
            " 18. Q\n",
            " 19. R\n",
            " 20. S\n",
            " 21. T\n",
            " 22. U\n",
            " 23. V\n",
            " 24. W\n",
            " 25. X\n",
            " 26. Y\n",
            " 27. Z\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load fourth model (CommonPhone - IPA phonemes)\n",
        "print(\"=\" * 80)\n",
        "print(\"MODEL 4: CommonPhone model (Direct loading)\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Model: pklumpp/Wav2Vec2_CommonPhone\")\n",
        "print(\"Note: This model uses IPA phonemes and is trained on Common Phone dataset\")\n",
        "print(\"which includes German speech. Should be better for German phoneme recognition.\")\n",
        "print()\n",
        "\n",
        "try:\n",
        "    from transformers import Wav2Vec2ForCTC, AutoModelForCTC\n",
        "    from huggingface_hub import hf_hub_download, list_repo_files\n",
        "    import json\n",
        "    from pathlib import Path\n",
        "    \n",
        "    model_name = \"pklumpp/Wav2Vec2_CommonPhone\"\n",
        "    print(f\"Loading CommonPhone model vocabulary: {model_name}\")\n",
        "    \n",
        "    vocab4 = {}\n",
        "    \n",
        "    # Method 1: Try to load vocab.json directly from Hugging Face\n",
        "    try:\n",
        "        print(\"Method 1: Attempting to download vocab.json from Hugging Face...\")\n",
        "        vocab_path = hf_hub_download(repo_id=model_name, filename=\"vocab.json\")\n",
        "        if vocab_path and Path(vocab_path).exists():\n",
        "            with open(vocab_path, 'r', encoding='utf-8') as f:\n",
        "                vocab4 = json.load(f)\n",
        "            print(f\"✓ Successfully loaded vocab.json with {len(vocab4)} tokens\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"vocab.json file not found after download\")\n",
        "    except Exception as e1:\n",
        "        print(f\"✗ Method 1 failed: {e1}\")\n",
        "        \n",
        "        # Method 2: Check what files are available in the repository\n",
        "        try:\n",
        "            print(\"Method 2: Checking available files in repository...\")\n",
        "            repo_files = list_repo_files(repo_id=model_name, repo_type=\"model\")\n",
        "            print(f\"Available files: {repo_files}\")\n",
        "            \n",
        "            # Try to find vocab file with different names\n",
        "            vocab_files = [f for f in repo_files if 'vocab' in f.lower() or 'tokenizer' in f.lower()]\n",
        "            if vocab_files:\n",
        "                print(f\"Found potential vocab files: {vocab_files}\")\n",
        "                for vocab_file in vocab_files:\n",
        "                    try:\n",
        "                        vocab_path = hf_hub_download(repo_id=model_name, filename=vocab_file)\n",
        "                        if vocab_path and Path(vocab_path).exists():\n",
        "                            with open(vocab_path, 'r', encoding='utf-8') as f:\n",
        "                                vocab4 = json.load(f)\n",
        "                            print(f\"✓ Successfully loaded {vocab_file} with {len(vocab4)} tokens\")\n",
        "                            break\n",
        "                    except:\n",
        "                        continue\n",
        "        except Exception as e2:\n",
        "            print(f\"✗ Method 2 failed: {e2}\")\n",
        "        \n",
        "        # Method 3: Try to load model and extract vocabulary from model config/state\n",
        "        if not vocab4:\n",
        "            try:\n",
        "                print(\"Method 3: Attempting to load model and extract vocabulary from model state...\")\n",
        "                # Try to load model config first\n",
        "                config_path = hf_hub_download(repo_id=model_name, filename=\"config.json\")\n",
        "                with open(config_path, 'r', encoding='utf-8') as f:\n",
        "                    model_config = json.load(f)\n",
        "                \n",
        "                vocab_size = model_config.get('vocab_size', None)\n",
        "                print(f\"Model vocab_size from config: {vocab_size}\")\n",
        "                \n",
        "                # Try to load the model (without processor) and see if we can get vocab from it\n",
        "                try:\n",
        "                    model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "                    # Check if model has vocab in its config or attributes\n",
        "                    if hasattr(model, 'config') and hasattr(model.config, 'vocab_size'):\n",
        "                        print(f\"Model loaded, vocab_size: {model.config.vocab_size}\")\n",
        "                        \n",
        "                        # Try to get vocab from model's lm_head if possible\n",
        "                        # Unfortunately, we can't directly extract token strings from model weights\n",
        "                        # But we can try to load tokenizer with ignore_mismatched_sizes\n",
        "                        print(\"Note: Cannot extract token strings directly from model weights.\")\n",
        "                        print(\"Trying Method 4: Install protobuf and load tokenizer...\")\n",
        "                except Exception as e3:\n",
        "                    print(f\"Could not load model: {e3}\")\n",
        "                    raise\n",
        "            except Exception as e3:\n",
        "                print(f\"✗ Method 3 failed: {e3}\")\n",
        "        \n",
        "        # Method 4: Try using AutoProcessor (may work even without vocab.json)\n",
        "        if not vocab4:\n",
        "            try:\n",
        "                print(\"Method 4: Attempting to load AutoProcessor...\")\n",
        "                from transformers import AutoProcessor\n",
        "                \n",
        "                processor = AutoProcessor.from_pretrained(model_name)\n",
        "                \n",
        "                if hasattr(processor, 'tokenizer'):\n",
        "                    tokenizer = processor.tokenizer\n",
        "                    if hasattr(tokenizer, 'get_vocab'):\n",
        "                        vocab4 = tokenizer.get_vocab()\n",
        "                        print(f\"✓ Successfully loaded vocabulary from AutoProcessor.tokenizer.get_vocab(): {len(vocab4)} tokens\")\n",
        "                    elif hasattr(tokenizer, 'vocab'):\n",
        "                        vocab4 = tokenizer.vocab\n",
        "                        print(f\"✓ Successfully loaded vocabulary from AutoProcessor.tokenizer.vocab: {len(vocab4)} tokens\")\n",
        "                    elif hasattr(tokenizer, 'convert_ids_to_tokens'):\n",
        "                        # Build vocab from convert_ids_to_tokens\n",
        "                        print(\"Building vocabulary from AutoProcessor.tokenizer.convert_ids_to_tokens()...\")\n",
        "                        vocab_size = getattr(tokenizer, 'vocab_size', 32)  # CommonPhone uses 32\n",
        "                        for i in range(vocab_size):\n",
        "                            try:\n",
        "                                token = tokenizer.convert_ids_to_tokens(i)\n",
        "                                if token is not None:\n",
        "                                    if isinstance(token, str):\n",
        "                                        vocab4[token] = i\n",
        "                                    else:\n",
        "                                        vocab4[str(token)] = i\n",
        "                            except (IndexError, KeyError, ValueError):\n",
        "                                continue\n",
        "                        print(f\"✓ Built vocabulary from AutoProcessor.tokenizer: {len(vocab4)} tokens\")\n",
        "                    else:\n",
        "                        raise AttributeError(\"AutoProcessor.tokenizer does not have expected methods\")\n",
        "                else:\n",
        "                    raise AttributeError(\"AutoProcessor does not have tokenizer attribute\")\n",
        "            except Exception as e4:\n",
        "                print(f\"✗ Method 4 failed: {e4}\")\n",
        "        \n",
        "        # Method 5: Try loading tokenizer from base model and adapt it\n",
        "        if not vocab4:\n",
        "            try:\n",
        "                print(\"Method 5: Attempting to load tokenizer from base model and extract vocab...\")\n",
        "                from transformers import Wav2Vec2CTCTokenizer\n",
        "                \n",
        "                # Try to load from base model that CommonPhone is based on\n",
        "                base_model = \"facebook/wav2vec2-large-xlsr-53\"\n",
        "                print(f\"  → Loading base model tokenizer: {base_model}\")\n",
        "                base_tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(base_model)\n",
        "                \n",
        "                # Get vocab from base model\n",
        "                if hasattr(base_tokenizer, 'get_vocab'):\n",
        "                    base_vocab = base_tokenizer.get_vocab()\n",
        "                    print(f\"  ✓ Base model vocab size: {len(base_vocab)}\")\n",
        "                    \n",
        "                    # CommonPhone uses vocab_size=32, so we need to map or extract first 32 tokens\n",
        "                    # But this won't give us the actual CommonPhone phonemes, just base model tokens\n",
        "                    print(\"  ⚠ Note: Base model vocab may not match CommonPhone phonemes exactly\")\n",
        "                    print(\"  → Trying to use base vocab as fallback...\")\n",
        "                    vocab4 = base_vocab\n",
        "                    print(f\"  ✓ Using base model vocab: {len(vocab4)} tokens (may not be accurate for CommonPhone)\")\n",
        "            except Exception as e5:\n",
        "                print(f\"✗ Method 5 failed: {e5}\")\n",
        "        \n",
        "        # Method 6: Try to read README.md for vocab information\n",
        "        if not vocab4:\n",
        "            try:\n",
        "                print(\"Method 6: Attempting to read README.md for vocabulary information...\")\n",
        "                readme_path = hf_hub_download(repo_id=model_name, filename=\"README.md\")\n",
        "                if readme_path and Path(readme_path).exists():\n",
        "                    with open(readme_path, 'r', encoding='utf-8') as f:\n",
        "                        readme_content = f.read()\n",
        "                    print(\"  ✓ README.md loaded\")\n",
        "                    # Look for vocab information in README (unlikely but worth trying)\n",
        "                    if 'vocab' in readme_content.lower() or 'phoneme' in readme_content.lower():\n",
        "                        print(\"  → Found vocab/phoneme mentions in README, but cannot extract automatically\")\n",
        "                    print(\"  ✗ Cannot extract vocab from README automatically\")\n",
        "            except Exception as e6:\n",
        "                print(f\"✗ Method 6 failed: {e6}\")\n",
        "        \n",
        "        # Final check\n",
        "        if not vocab4:\n",
        "            raise RuntimeError(\n",
        "                f\"Failed to load vocabulary from CommonPhone model '{model_name}'. \"\n",
        "                f\"Tried all methods:\\n\"\n",
        "                f\"  1. vocab.json download (404 - file not found)\\n\"\n",
        "                f\"  2. Repository file listing (no vocab files found)\\n\"\n",
        "                f\"  3. Model config (vocab_size=32 found, but no token strings)\\n\"\n",
        "                f\"  4. AutoProcessor (failed)\\n\"\n",
        "                f\"  5. Base model tokenizer (failed or not accurate)\\n\"\n",
        "                f\"  6. README.md (cannot extract automatically)\\n\\n\"\n",
        "                f\"The model repository does not contain vocab.json or tokenizer files.\\n\"\n",
        "                f\"This appears to be a limitation of how the model was uploaded to Hugging Face.\\n\"\n",
        "                f\"Model repository: https://huggingface.co/{model_name}\\n\"\n",
        "                f\"Available files: {repo_files if 'repo_files' in locals() else 'unknown'}\"\n",
        "            )\n",
        "    \n",
        "    if not vocab4:\n",
        "        raise RuntimeError(\"Vocabulary is empty after all loading attempts\")\n",
        "    \n",
        "    phonemes4 = filter_phonemes(vocab4)\n",
        "    \n",
        "    print(f\"\\nVocabulary size: {len(vocab4)}\")\n",
        "    print(f\"Number of phonemes (after filtering): {len(phonemes4)}\")\n",
        "    print()\n",
        "    print(\"All phonemes from model 4:\")\n",
        "    print(\"-\" * 80)\n",
        "    for i, phoneme in enumerate(phonemes4, 1):\n",
        "        print(f\"{i:3d}. {phoneme}\")\n",
        "    print()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Error: Could not load vocabulary from CommonPhone model\")\n",
        "    print(f\"Error details: {e}\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUMMARY:\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"The CommonPhone model repository does not contain vocabulary files (vocab.json, tokenizer files).\")\n",
        "    print(\"This is a limitation of how the model was uploaded to Hugging Face.\")\n",
        "    print(\"\\nThe model itself is available and can be used for inference, but the vocabulary\")\n",
        "    print(\"(list of phonemes) cannot be extracted automatically from the repository.\")\n",
        "    print(\"\\nTo get the vocabulary, you would need to:\")\n",
        "    print(\"  1. Contact the model author (pklumpp) to request vocab.json\")\n",
        "    print(\"  2. Check the original paper/dataset documentation\")\n",
        "    print(\"  3. Use the model for inference and observe which phonemes it outputs\")\n",
        "    print(\"\\nFor now, we'll continue with the other three models for comparison.\")\n",
        "    print(\"=\"*80)\n",
        "    phonemes4 = []"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "MODEL 4: CommonPhone model (Direct loading)\n",
            "================================================================================\n",
            "Model: pklumpp/Wav2Vec2_CommonPhone\n",
            "Note: This model uses IPA phonemes and is trained on Common Phone dataset\n",
            "which includes German speech. Should be better for German phoneme recognition.\n",
            "\n",
            "Loading CommonPhone model vocabulary: pklumpp/Wav2Vec2_CommonPhone\n",
            "Method 1: Attempting to download vocab.json from Hugging Face...\n",
            "✗ Method 1 failed: 404 Client Error. (Request ID: Root=1-695f868d-62f189ca0248367b111f0fba;9abe477e-6715-40cb-b7c1-4d4ba8f78a8f)\n",
            "\n",
            "Entry Not Found for url: https://huggingface.co/pklumpp/Wav2Vec2_CommonPhone/resolve/main/vocab.json.\n",
            "Method 2: Checking available files in repository...\n",
            "Available files: ['.gitattributes', 'README.md', 'config.json', 'model.safetensors', 'pytorch_model.bin']\n",
            "Method 3: Attempting to load model and extract vocabulary from model state...\n",
            "Model vocab_size from config: None\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at pklumpp/Wav2Vec2_CommonPhone and are newly initialized: ['lm_head.bias', 'lm_head.weight', 'wav2vec2.encoder.layer_norm.bias', 'wav2vec2.encoder.layer_norm.weight', 'wav2vec2.encoder.layers.0.attention.k_proj.bias', 'wav2vec2.encoder.layers.0.attention.k_proj.weight', 'wav2vec2.encoder.layers.0.attention.out_proj.bias', 'wav2vec2.encoder.layers.0.attention.out_proj.weight', 'wav2vec2.encoder.layers.0.attention.q_proj.bias', 'wav2vec2.encoder.layers.0.attention.q_proj.weight', 'wav2vec2.encoder.layers.0.attention.v_proj.bias', 'wav2vec2.encoder.layers.0.attention.v_proj.weight', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.0.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.0.final_layer_norm.bias', 'wav2vec2.encoder.layers.0.final_layer_norm.weight', 'wav2vec2.encoder.layers.0.layer_norm.bias', 'wav2vec2.encoder.layers.0.layer_norm.weight', 'wav2vec2.encoder.layers.1.attention.k_proj.bias', 'wav2vec2.encoder.layers.1.attention.k_proj.weight', 'wav2vec2.encoder.layers.1.attention.out_proj.bias', 'wav2vec2.encoder.layers.1.attention.out_proj.weight', 'wav2vec2.encoder.layers.1.attention.q_proj.bias', 'wav2vec2.encoder.layers.1.attention.q_proj.weight', 'wav2vec2.encoder.layers.1.attention.v_proj.bias', 'wav2vec2.encoder.layers.1.attention.v_proj.weight', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.1.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.1.final_layer_norm.bias', 'wav2vec2.encoder.layers.1.final_layer_norm.weight', 'wav2vec2.encoder.layers.1.layer_norm.bias', 'wav2vec2.encoder.layers.1.layer_norm.weight', 'wav2vec2.encoder.layers.10.attention.k_proj.bias', 'wav2vec2.encoder.layers.10.attention.k_proj.weight', 'wav2vec2.encoder.layers.10.attention.out_proj.bias', 'wav2vec2.encoder.layers.10.attention.out_proj.weight', 'wav2vec2.encoder.layers.10.attention.q_proj.bias', 'wav2vec2.encoder.layers.10.attention.q_proj.weight', 'wav2vec2.encoder.layers.10.attention.v_proj.bias', 'wav2vec2.encoder.layers.10.attention.v_proj.weight', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.10.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.10.final_layer_norm.bias', 'wav2vec2.encoder.layers.10.final_layer_norm.weight', 'wav2vec2.encoder.layers.10.layer_norm.bias', 'wav2vec2.encoder.layers.10.layer_norm.weight', 'wav2vec2.encoder.layers.11.attention.k_proj.bias', 'wav2vec2.encoder.layers.11.attention.k_proj.weight', 'wav2vec2.encoder.layers.11.attention.out_proj.bias', 'wav2vec2.encoder.layers.11.attention.out_proj.weight', 'wav2vec2.encoder.layers.11.attention.q_proj.bias', 'wav2vec2.encoder.layers.11.attention.q_proj.weight', 'wav2vec2.encoder.layers.11.attention.v_proj.bias', 'wav2vec2.encoder.layers.11.attention.v_proj.weight', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.11.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.11.final_layer_norm.bias', 'wav2vec2.encoder.layers.11.final_layer_norm.weight', 'wav2vec2.encoder.layers.11.layer_norm.bias', 'wav2vec2.encoder.layers.11.layer_norm.weight', 'wav2vec2.encoder.layers.2.attention.k_proj.bias', 'wav2vec2.encoder.layers.2.attention.k_proj.weight', 'wav2vec2.encoder.layers.2.attention.out_proj.bias', 'wav2vec2.encoder.layers.2.attention.out_proj.weight', 'wav2vec2.encoder.layers.2.attention.q_proj.bias', 'wav2vec2.encoder.layers.2.attention.q_proj.weight', 'wav2vec2.encoder.layers.2.attention.v_proj.bias', 'wav2vec2.encoder.layers.2.attention.v_proj.weight', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.2.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.2.final_layer_norm.bias', 'wav2vec2.encoder.layers.2.final_layer_norm.weight', 'wav2vec2.encoder.layers.2.layer_norm.bias', 'wav2vec2.encoder.layers.2.layer_norm.weight', 'wav2vec2.encoder.layers.3.attention.k_proj.bias', 'wav2vec2.encoder.layers.3.attention.k_proj.weight', 'wav2vec2.encoder.layers.3.attention.out_proj.bias', 'wav2vec2.encoder.layers.3.attention.out_proj.weight', 'wav2vec2.encoder.layers.3.attention.q_proj.bias', 'wav2vec2.encoder.layers.3.attention.q_proj.weight', 'wav2vec2.encoder.layers.3.attention.v_proj.bias', 'wav2vec2.encoder.layers.3.attention.v_proj.weight', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.3.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.3.final_layer_norm.bias', 'wav2vec2.encoder.layers.3.final_layer_norm.weight', 'wav2vec2.encoder.layers.3.layer_norm.bias', 'wav2vec2.encoder.layers.3.layer_norm.weight', 'wav2vec2.encoder.layers.4.attention.k_proj.bias', 'wav2vec2.encoder.layers.4.attention.k_proj.weight', 'wav2vec2.encoder.layers.4.attention.out_proj.bias', 'wav2vec2.encoder.layers.4.attention.out_proj.weight', 'wav2vec2.encoder.layers.4.attention.q_proj.bias', 'wav2vec2.encoder.layers.4.attention.q_proj.weight', 'wav2vec2.encoder.layers.4.attention.v_proj.bias', 'wav2vec2.encoder.layers.4.attention.v_proj.weight', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.4.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.4.final_layer_norm.bias', 'wav2vec2.encoder.layers.4.final_layer_norm.weight', 'wav2vec2.encoder.layers.4.layer_norm.bias', 'wav2vec2.encoder.layers.4.layer_norm.weight', 'wav2vec2.encoder.layers.5.attention.k_proj.bias', 'wav2vec2.encoder.layers.5.attention.k_proj.weight', 'wav2vec2.encoder.layers.5.attention.out_proj.bias', 'wav2vec2.encoder.layers.5.attention.out_proj.weight', 'wav2vec2.encoder.layers.5.attention.q_proj.bias', 'wav2vec2.encoder.layers.5.attention.q_proj.weight', 'wav2vec2.encoder.layers.5.attention.v_proj.bias', 'wav2vec2.encoder.layers.5.attention.v_proj.weight', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.5.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.5.final_layer_norm.bias', 'wav2vec2.encoder.layers.5.final_layer_norm.weight', 'wav2vec2.encoder.layers.5.layer_norm.bias', 'wav2vec2.encoder.layers.5.layer_norm.weight', 'wav2vec2.encoder.layers.6.attention.k_proj.bias', 'wav2vec2.encoder.layers.6.attention.k_proj.weight', 'wav2vec2.encoder.layers.6.attention.out_proj.bias', 'wav2vec2.encoder.layers.6.attention.out_proj.weight', 'wav2vec2.encoder.layers.6.attention.q_proj.bias', 'wav2vec2.encoder.layers.6.attention.q_proj.weight', 'wav2vec2.encoder.layers.6.attention.v_proj.bias', 'wav2vec2.encoder.layers.6.attention.v_proj.weight', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.6.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.6.final_layer_norm.bias', 'wav2vec2.encoder.layers.6.final_layer_norm.weight', 'wav2vec2.encoder.layers.6.layer_norm.bias', 'wav2vec2.encoder.layers.6.layer_norm.weight', 'wav2vec2.encoder.layers.7.attention.k_proj.bias', 'wav2vec2.encoder.layers.7.attention.k_proj.weight', 'wav2vec2.encoder.layers.7.attention.out_proj.bias', 'wav2vec2.encoder.layers.7.attention.out_proj.weight', 'wav2vec2.encoder.layers.7.attention.q_proj.bias', 'wav2vec2.encoder.layers.7.attention.q_proj.weight', 'wav2vec2.encoder.layers.7.attention.v_proj.bias', 'wav2vec2.encoder.layers.7.attention.v_proj.weight', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.7.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.7.final_layer_norm.bias', 'wav2vec2.encoder.layers.7.final_layer_norm.weight', 'wav2vec2.encoder.layers.7.layer_norm.bias', 'wav2vec2.encoder.layers.7.layer_norm.weight', 'wav2vec2.encoder.layers.8.attention.k_proj.bias', 'wav2vec2.encoder.layers.8.attention.k_proj.weight', 'wav2vec2.encoder.layers.8.attention.out_proj.bias', 'wav2vec2.encoder.layers.8.attention.out_proj.weight', 'wav2vec2.encoder.layers.8.attention.q_proj.bias', 'wav2vec2.encoder.layers.8.attention.q_proj.weight', 'wav2vec2.encoder.layers.8.attention.v_proj.bias', 'wav2vec2.encoder.layers.8.attention.v_proj.weight', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.8.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.8.final_layer_norm.bias', 'wav2vec2.encoder.layers.8.final_layer_norm.weight', 'wav2vec2.encoder.layers.8.layer_norm.bias', 'wav2vec2.encoder.layers.8.layer_norm.weight', 'wav2vec2.encoder.layers.9.attention.k_proj.bias', 'wav2vec2.encoder.layers.9.attention.k_proj.weight', 'wav2vec2.encoder.layers.9.attention.out_proj.bias', 'wav2vec2.encoder.layers.9.attention.out_proj.weight', 'wav2vec2.encoder.layers.9.attention.q_proj.bias', 'wav2vec2.encoder.layers.9.attention.q_proj.weight', 'wav2vec2.encoder.layers.9.attention.v_proj.bias', 'wav2vec2.encoder.layers.9.attention.v_proj.weight', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias', 'wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.bias', 'wav2vec2.encoder.layers.9.feed_forward.output_dense.weight', 'wav2vec2.encoder.layers.9.final_layer_norm.bias', 'wav2vec2.encoder.layers.9.final_layer_norm.weight', 'wav2vec2.encoder.layers.9.layer_norm.bias', 'wav2vec2.encoder.layers.9.layer_norm.weight', 'wav2vec2.encoder.pos_conv_embed.conv.bias', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.feature_extractor.conv_layers.0.conv.weight', 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.1.conv.weight', 'wav2vec2.feature_extractor.conv_layers.2.conv.weight', 'wav2vec2.feature_extractor.conv_layers.3.conv.weight', 'wav2vec2.feature_extractor.conv_layers.4.conv.weight', 'wav2vec2.feature_extractor.conv_layers.5.conv.weight', 'wav2vec2.feature_extractor.conv_layers.6.conv.weight', 'wav2vec2.feature_projection.layer_norm.bias', 'wav2vec2.feature_projection.layer_norm.weight', 'wav2vec2.feature_projection.projection.bias', 'wav2vec2.feature_projection.projection.weight', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Model loaded, vocab_size: 32\n",
            "Note: Cannot extract token strings directly from model weights.\n",
            "Trying Method 4: Install protobuf and load tokenizer...\n",
            "Method 4: Attempting to load AutoProcessor...\n",
            "✗ Method 4 failed: Unrecognized model in pklumpp/Wav2Vec2_CommonPhone. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: aimv2, aimv2_vision_model, albert, align, altclip, apertus, arcee, aria, aria_text, audio-spectrogram-transformer, autoformer, aya_vision, bamba, bark, bart, beit, bert, bert-generation, big_bird, bigbird_pegasus, biogpt, bit, bitnet, blenderbot, blenderbot-small, blip, blip-2, blip_2_qformer, bloom, blt, bridgetower, bros, camembert, canine, chameleon, chinese_clip, chinese_clip_vision_model, clap, clip, clip_text_model, clip_vision_model, clipseg, clvp, code_llama, codegen, cohere, cohere2, cohere2_vision, colpali, colqwen2, conditional_detr, convbert, convnext, convnextv2, cpmant, csm, ctrl, cvt, d_fine, dab-detr, dac, data2vec-audio, data2vec-text, data2vec-vision, dbrx, deberta, deberta-v2, decision_transformer, deepseek_v2, deepseek_v3, deepseek_vl, deepseek_vl_hybrid, deformable_detr, deit, depth_anything, depth_pro, deta, detr, dia, diffllama, dinat, dinov2, dinov2_with_registers, dinov3_convnext, dinov3_vit, distilbert, doge, donut-swin, dots1, dpr, dpt, edgetam, edgetam_video, edgetam_vision_model, efficientformer, efficientloftr, efficientnet, electra, emu3, encodec, encoder-decoder, eomt, ernie, ernie4_5, ernie4_5_moe, ernie_m, esm, evolla, exaone4, falcon, falcon_h1, falcon_mamba, fastspeech2_conformer, fastspeech2_conformer_with_hifigan, flaubert, flava, flex_olmo, florence2, fnet, focalnet, fsmt, funnel, fuyu, gemma, gemma2, gemma3, gemma3_text, gemma3n, gemma3n_audio, gemma3n_text, gemma3n_vision, git, glm, glm4, glm4_moe, glm4v, glm4v_moe, glm4v_moe_text, glm4v_text, glpn, got_ocr2, gpt-sw3, gpt2, gpt_bigcode, gpt_neo, gpt_neox, gpt_neox_japanese, gpt_oss, gptj, gptsan-japanese, granite, granite_speech, granitemoe, granitemoehybrid, granitemoeshared, granitevision, graphormer, grounding-dino, groupvit, helium, hgnet_v2, hiera, hubert, hunyuan_v1_dense, hunyuan_v1_moe, ibert, idefics, idefics2, idefics3, idefics3_vision, ijepa, imagegpt, informer, instructblip, instructblipvideo, internvl, internvl_vision, jamba, janus, jetmoe, jukebox, kosmos-2, kosmos-2.5, kyutai_speech_to_text, layoutlm, layoutlmv2, layoutlmv3, led, levit, lfm2, lfm2_vl, lightglue, lilt, llama, llama4, llama4_text, llava, llava_next, llava_next_video, llava_onevision, longcat_flash, longformer, longt5, luke, lxmert, m2m_100, mamba, mamba2, marian, markuplm, mask2former, maskformer, maskformer-swin, mbart, mctct, mega, megatron-bert, metaclip_2, mgp-str, mimi, minimax, ministral, mistral, mistral3, mixtral, mlcd, mllama, mm-grounding-dino, mobilebert, mobilenet_v1, mobilenet_v2, mobilevit, mobilevitv2, modernbert, modernbert-decoder, moonshine, moshi, mpnet, mpt, mra, mt5, musicgen, musicgen_melody, mvp, nat, nemotron, nezha, nllb-moe, nougat, nystromformer, olmo, olmo2, olmo3, olmoe, omdet-turbo, oneformer, open-llama, openai-gpt, opt, ovis2, owlv2, owlvit, paligemma, parakeet_ctc, parakeet_encoder, patchtsmixer, patchtst, pegasus, pegasus_x, perceiver, perception_encoder, perception_lm, persimmon, phi, phi3, phi4_multimodal, phimoe, pix2struct, pixtral, plbart, poolformer, pop2piano, prompt_depth_anything, prophetnet, pvt, pvt_v2, qdqbert, qwen2, qwen2_5_omni, qwen2_5_vl, qwen2_5_vl_text, qwen2_audio, qwen2_audio_encoder, qwen2_moe, qwen2_vl, qwen2_vl_text, qwen3, qwen3_moe, qwen3_next, qwen3_omni_moe, qwen3_vl, qwen3_vl_moe, qwen3_vl_moe_text, qwen3_vl_text, rag, realm, recurrent_gemma, reformer, regnet, rembert, resnet, retribert, roberta, roberta-prelayernorm, roc_bert, roformer, rt_detr, rt_detr_resnet, rt_detr_v2, rwkv, sam, sam2, sam2_hiera_det_model, sam2_video, sam2_vision_model, sam_hq, sam_hq_vision_model, sam_vision_model, seamless_m4t, seamless_m4t_v2, seed_oss, segformer, seggpt, sew, sew-d, shieldgemma2, siglip, siglip2, siglip2_vision_model, siglip_vision_model, smollm3, smolvlm, smolvlm_vision, speech-encoder-decoder, speech_to_text, speech_to_text_2, speecht5, splinter, squeezebert, stablelm, starcoder2, superglue, superpoint, swiftformer, swin, swin2sr, swinv2, switch_transformers, t5, t5gemma, table-transformer, tapas, textnet, time_series_transformer, timesfm, timesformer, timm_backbone, timm_wrapper, trajectory_transformer, transfo-xl, trocr, tvlt, tvp, udop, umt5, unispeech, unispeech-sat, univnet, upernet, van, vaultgemma, video_llava, videomae, vilt, vipllava, vision-encoder-decoder, vision-text-dual-encoder, visual_bert, vit, vit_hybrid, vit_mae, vit_msn, vitdet, vitmatte, vitpose, vitpose_backbone, vits, vivit, vjepa2, voxtral, voxtral_encoder, wav2vec2, wav2vec2-bert, wav2vec2-conformer, wavlm, whisper, xclip, xcodec, xglm, xlm, xlm-prophetnet, xlm-roberta, xlm-roberta-xl, xlnet, xlstm, xmod, yolos, yoso, zamba, zamba2, zoedepth\n",
            "Method 5: Attempting to load tokenizer from base model and extract vocab...\n",
            "  → Loading base model tokenizer: facebook/wav2vec2-large-xlsr-53\n",
            "✗ Method 5 failed: expected str, bytes or os.PathLike object, not NoneType\n",
            "Method 6: Attempting to read README.md for vocabulary information...\n",
            "  ✓ README.md loaded\n",
            "  ✗ Cannot extract vocab from README automatically\n",
            "\n",
            "✗ Error: Could not load vocabulary from CommonPhone model\n",
            "Error details: Failed to load vocabulary from CommonPhone model 'pklumpp/Wav2Vec2_CommonPhone'. Tried all methods:\n",
            "  1. vocab.json download (404 - file not found)\n",
            "  2. Repository file listing (no vocab files found)\n",
            "  3. Model config (vocab_size=32 found, but no token strings)\n",
            "  4. AutoProcessor (failed)\n",
            "  5. Base model tokenizer (failed or not accurate)\n",
            "  6. README.md (cannot extract automatically)\n",
            "\n",
            "The model repository does not contain vocab.json or tokenizer files.\n",
            "This appears to be a limitation of how the model was uploaded to Hugging Face.\n",
            "Model repository: https://huggingface.co/pklumpp/Wav2Vec2_CommonPhone\n",
            "Available files: ['.gitattributes', 'README.md', 'config.json', 'model.safetensors', 'pytorch_model.bin']\n",
            "\n",
            "================================================================================\n",
            "SUMMARY:\n",
            "================================================================================\n",
            "The CommonPhone model repository does not contain vocabulary files (vocab.json, tokenizer files).\n",
            "This is a limitation of how the model was uploaded to Hugging Face.\n",
            "\n",
            "The model itself is available and can be used for inference, but the vocabulary\n",
            "(list of phonemes) cannot be extracted automatically from the repository.\n",
            "\n",
            "To get the vocabulary, you would need to:\n",
            "  1. Contact the model author (pklumpp) to request vocab.json\n",
            "  2. Check the original paper/dataset documentation\n",
            "  3. Use the model for inference and observe which phonemes it outputs\n",
            "\n",
            "For now, we'll continue with the other three models for comparison.\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compare phonemes from all four models\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPARISON OF PHONEMES FROM ALL FOUR MODELS\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Convert to sets for comparison\n",
        "set1 = set(phonemes1) if 'phonemes1' in locals() else set()\n",
        "set2 = set(phonemes2) if 'phonemes2' in locals() else set()\n",
        "set3 = set(phonemes3) if 'phonemes3' in locals() else set()\n",
        "set4 = set(phonemes4) if 'phonemes4' in locals() else set()\n",
        "\n",
        "print(f\"Model 1 ({config.MODEL_NAME}):\")\n",
        "print(f\"  - Total phonemes: {len(set1)}\")\n",
        "print(f\"  - Format: ARPABET (English phoneme system)\")\n",
        "print(f\"  - Phonemes: {sorted(set1)}\")\n",
        "print()\n",
        "\n",
        "print(f\"Model 2 ({config.WAV2VEC2_PHONEME_MODEL_NAME}):\")\n",
        "print(f\"  - Total phonemes: {len(set2)}\")\n",
        "print(f\"  - Format: ARPABET (English phoneme system)\")\n",
        "print(f\"  - Phonemes: {sorted(set2)}\")\n",
        "print()\n",
        "\n",
        "print(f\"Model 3 ({config.WAV2VEC2_LARGE_MODEL_NAME}):\")\n",
        "print(f\"  - Total phonemes: {len(set3)}\")\n",
        "print(f\"  - Format: Latin alphabet (letters, not phonemes)\")\n",
        "print(f\"  - Phonemes: {sorted(set3)}\")\n",
        "print()\n",
        "\n",
        "print(f\"Model 4 (pklumpp/Wav2Vec2_CommonPhone):\")\n",
        "print(f\"  - Total phonemes: {len(set4)}\")\n",
        "print(f\"  - Format: IPA (International Phonetic Alphabet)\")\n",
        "if len(set4) == 0:\n",
        "    print(f\"  - Status: Vocabulary not available (model repository lacks vocab.json/tokenizer files)\")\n",
        "    print(f\"  - Note: Model can be used for inference, but phoneme list cannot be extracted\")\n",
        "else:\n",
        "    print(f\"  - Phonemes: {sorted(set4)}\")\n",
        "print()\n",
        "\n",
        "# Common phonemes\n",
        "all_phonemes = set1 | set2 | set3 | set4\n",
        "common_all = set1 & set2 & set3 & set4\n",
        "common_arpabet = set1 & set2  # Models 1 and 2 use same ARPABET\n",
        "only_model1 = set1 - set2 - set3 - set4\n",
        "only_model2 = set2 - set1 - set3 - set4\n",
        "only_model3 = set3 - set1 - set2 - set4\n",
        "only_model4 = set4 - set1 - set2 - set3\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ANALYSIS:\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total unique phonemes across all models: {len(all_phonemes)}\")\n",
        "print(f\"Common phonemes (present in all four): {len(common_all)}\")\n",
        "print(f\"Common phonemes (ARPABET models 1 & 2): {len(common_arpabet)}\")\n",
        "print(f\"Only in model 1: {len(only_model1)}\")\n",
        "print(f\"Only in model 2: {len(only_model2)}\")\n",
        "print(f\"Only in model 3: {len(only_model3)}\")\n",
        "print(f\"Only in model 4 (CommonPhone IPA): {len(only_model4)}\")\n",
        "print()\n",
        "\n",
        "if common_all:\n",
        "    print(\"Common phonemes (present in all four models):\")\n",
        "    print(sorted(common_all))\n",
        "    print()\n",
        "\n",
        "if common_arpabet:\n",
        "    print(\"Common phonemes (ARPABET models 1 & 2):\")\n",
        "    print(sorted(common_arpabet))\n",
        "    print()\n",
        "\n",
        "if only_model1:\n",
        "    print(\"Phonemes only in model 1:\")\n",
        "    print(sorted(only_model1))\n",
        "    print()\n",
        "\n",
        "if only_model2:\n",
        "    print(\"Phonemes only in model 2:\")\n",
        "    print(sorted(only_model2))\n",
        "    print()\n",
        "\n",
        "if only_model3:\n",
        "    print(\"Phonemes only in model 3:\")\n",
        "    print(sorted(only_model3))\n",
        "    print()\n",
        "\n",
        "if only_model4:\n",
        "    print(\"Phonemes only in model 4 (CommonPhone IPA):\")\n",
        "    print(sorted(only_model4))\n",
        "    print()\n",
        "\n",
        "# Check which German IPA phonemes are present in CommonPhone\n",
        "print(\"=\" * 80)\n",
        "print(\"GERMAN IPA PHONEMES CHECK:\")\n",
        "print(\"=\" * 80)\n",
        "german_ipa_phonemes = set(config.GERMAN_IPA_PHONEMES)\n",
        "\n",
        "if len(set4) > 0:\n",
        "    commonphone_has_german = german_ipa_phonemes & set4\n",
        "    commonphone_missing_german = german_ipa_phonemes - set4\n",
        "    \n",
        "    print(f\"German IPA phonemes defined in config: {len(german_ipa_phonemes)}\")\n",
        "    print(f\"Present in CommonPhone model: {len(commonphone_has_german)}\")\n",
        "    print(f\"Missing in CommonPhone model: {len(commonphone_missing_german)}\")\n",
        "    print()\n",
        "    \n",
        "    if commonphone_has_german:\n",
        "        print(\"German IPA phonemes found in CommonPhone:\")\n",
        "        print(sorted(commonphone_has_german))\n",
        "        print()\n",
        "    \n",
        "    if commonphone_missing_german:\n",
        "        print(\"German IPA phonemes missing in CommonPhone:\")\n",
        "        print(sorted(commonphone_missing_german))\n",
        "        print()\n",
        "else:\n",
        "    print(f\"German IPA phonemes defined in config: {len(german_ipa_phonemes)}\")\n",
        "    print(\"⚠ Cannot check CommonPhone model: vocabulary not available\")\n",
        "    print(\"\\nGerman IPA phonemes that should be checked in CommonPhone:\")\n",
        "    print(sorted(german_ipa_phonemes))\n",
        "    print()\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "COMPARISON OF PHONEMES FROM ALL FOUR MODELS\n",
            "================================================================================\n",
            "\n",
            "Model 1 (vitouphy/wav2vec2-xls-r-300m-phoneme):\n",
            "  - Total phonemes: 38\n",
            "  - Format: ARPABET (English phoneme system)\n",
            "  - Phonemes: ['aa', 'ae', 'ah', 'aw', 'ay', 'b', 'ch', 'd', 'dh', 'dx', 'eh', 'er', 'ey', 'f', 'g', 'hh', 'ih', 'iy', 'jh', 'k', 'l', 'm', 'n', 'ng', 'ow', 'oy', 'p', 'r', 's', 'sh', 't', 'th', 'uh', 'uw', 'v', 'w', 'y', 'z']\n",
            "\n",
            "Model 2 (vitouphy/wav2vec2-xls-r-300m-phoneme):\n",
            "  - Total phonemes: 38\n",
            "  - Format: ARPABET (English phoneme system)\n",
            "  - Phonemes: ['aa', 'ae', 'ah', 'aw', 'ay', 'b', 'ch', 'd', 'dh', 'dx', 'eh', 'er', 'ey', 'f', 'g', 'hh', 'ih', 'iy', 'jh', 'k', 'l', 'm', 'n', 'ng', 'ow', 'oy', 'p', 'r', 's', 'sh', 't', 'th', 'uh', 'uw', 'v', 'w', 'y', 'z']\n",
            "\n",
            "Model 3 (facebook/wav2vec2-large-960h-lv60-self):\n",
            "  - Total phonemes: 27\n",
            "  - Format: Latin alphabet (letters, not phonemes)\n",
            "  - Phonemes: [\"'\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
            "\n",
            "Model 4 (pklumpp/Wav2Vec2_CommonPhone):\n",
            "  - Total phonemes: 0\n",
            "  - Format: IPA (International Phonetic Alphabet)\n",
            "  - Status: Vocabulary not available (model repository lacks vocab.json/tokenizer files)\n",
            "  - Note: Model can be used for inference, but phoneme list cannot be extracted\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS:\n",
            "================================================================================\n",
            "Total unique phonemes across all models: 65\n",
            "Common phonemes (present in all four): 0\n",
            "Common phonemes (ARPABET models 1 & 2): 38\n",
            "Only in model 1: 0\n",
            "Only in model 2: 0\n",
            "Only in model 3: 27\n",
            "Only in model 4 (CommonPhone IPA): 0\n",
            "\n",
            "Common phonemes (ARPABET models 1 & 2):\n",
            "['aa', 'ae', 'ah', 'aw', 'ay', 'b', 'ch', 'd', 'dh', 'dx', 'eh', 'er', 'ey', 'f', 'g', 'hh', 'ih', 'iy', 'jh', 'k', 'l', 'm', 'n', 'ng', 'ow', 'oy', 'p', 'r', 's', 'sh', 't', 'th', 'uh', 'uw', 'v', 'w', 'y', 'z']\n",
            "\n",
            "Phonemes only in model 3:\n",
            "[\"'\", 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
            "\n",
            "================================================================================\n",
            "GERMAN IPA PHONEMES CHECK:\n",
            "================================================================================\n",
            "German IPA phonemes defined in config: 51\n",
            "⚠ Cannot check CommonPhone model: vocabulary not available\n",
            "\n",
            "German IPA phonemes that should be checked in CommonPhone:\n",
            "['a', 'aɪ̯', 'aʊ̯', 'aː', 'b', 'd', 'dʒ', 'e', 'eː', 'f', 'g', 'h', 'i', 'iː', 'j', 'k', 'kʰ', 'l', 'm', 'n', 'o', 'oː', 'p', 'pf', 's', 't', 'ts', 'tʃ', 'u', 'uː', 'v', 'x', 'y', 'yː', 'z', 'ç', 'ø', 'øː', 'ŋ', 'œ', 'ɐ', 'ɔ', 'ɔʏ̯', 'ə', 'ɛ', 'ɪ', 'ʁ', 'ʃ', 'ʊ', 'ʏ', 'ʒ']\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv_pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}